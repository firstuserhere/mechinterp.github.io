<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mechinterptitle>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <main>
    <h1>MechInterp</h1>
    <p>
      Mechanistic interpretability (often shortened to mechinterp) reframes neural networks not as inscrutable black boxes, but as computational artifacts to be reverse-engineered. Neural networks, from this viewpoint, resemble complex computational circuits: composed of distinct modules and subroutines whose operations can be teased apart and explicitly understood. By systematically probing models, researchers in mechanistic interpretability seek to identify precisely which algorithms a neural network has learned, how these algorithms are encoded in its parameters, and exactly how computation flows from inputs to outputs.
    </p>
  </main>
</body>
</html>